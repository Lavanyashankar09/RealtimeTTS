{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0e6f2d-6be2-4fc4-8d08-775b0defd63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/orpheus_tts/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 1 files: 100%|█████████████████████████| 1/1 [00:00<00:00, 3530.56it/s]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "import torch\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# Load models\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "# Load speaker embeddings\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "async def generate_audio_chunks_with_char_timestamps(text: str, chunk_size=4):\n",
    "    words = text.strip().split()\n",
    "    char_pos = 0\n",
    "\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk_words = words[i:i+chunk_size]\n",
    "        chunk_text = \" \".join(chunk_words)\n",
    "\n",
    "        # Generate speech\n",
    "        inputs = processor(text=chunk_text, return_tensors=\"pt\")\n",
    "        speech_chunk = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "\n",
    "        # Approximate character-level timings\n",
    "        num_chars = len(chunk_text)\n",
    "        total_samples = speech_chunk.shape[0]\n",
    "        duration_ms = total_samples / 16000 * 1000  # assuming 16kHz\n",
    "\n",
    "        # simple proportional distribution\n",
    "        char_durations_ms = [duration_ms / num_chars] * num_chars\n",
    "        char_start_times_ms = np.cumsum([0]+char_durations_ms[:-1]).tolist()\n",
    "\n",
    "        yield {\n",
    "            \"chars\": list(chunk_text),\n",
    "            \"char_start_times_ms\": [round(t) for t in char_start_times_ms],\n",
    "            \"char_durations_ms\": [round(d) for d in char_durations_ms],\n",
    "            \"audio\": speech_chunk\n",
    "        }\n",
    "\n",
    "        char_pos += len(chunk_text) + 1  # +1 for space\n",
    "        await asyncio.sleep(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ac8511-e7e0-481b-a137-e0c5e116d438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk chars: This is an example\n",
      "Start times (ms): [0, 78, 156, 235, 313, 391, 469, 548, 626, 704, 782, 860, 939, 1017, 1095, 1173, 1252, 1330]\n",
      "Durations (ms): [78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78]\n",
      "Chunk chars: of alignment data.\n",
      "Start times (ms): [0, 69, 139, 208, 277, 347, 416, 485, 555, 624, 693, 763, 832, 901, 971, 1040, 1109, 1179]\n",
      "Durations (ms): [69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69]\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "async def play_with_char_alignment(text):\n",
    "    async for chunk_data in generate_audio_chunks_with_char_timestamps(text):\n",
    "        audio_np = chunk_data[\"audio\"].numpy()\n",
    "        print(\"Chunk chars:\", \"\".join(chunk_data[\"chars\"]))\n",
    "        print(\"Start times (ms):\", chunk_data[\"char_start_times_ms\"])\n",
    "        print(\"Durations (ms):\", chunk_data[\"char_durations_ms\"])\n",
    "        sd.play(audio_np, samplerate=16000, blocking=True)\n",
    "\n",
    "# Run in notebook\n",
    "await play_with_char_alignment(\"This is an example of alignment data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33659713-e15f-4d35-a49d-9a95aaa2072e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orpheus TTS",
   "language": "python",
   "name": "orpheus_tts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
