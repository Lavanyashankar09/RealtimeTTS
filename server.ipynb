{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc46d28b-b78e-4000-894e-bd23659ef6b3",
   "metadata": {},
   "source": [
    "### SpeechT5 Model\n",
    "\n",
    "- **What it is**:  \n",
    "  SpeechT5 is a **transformer-based model** from Microsoft designed for both **text-to-speech (TTS)** and **speech-to-text (ASR)** tasks.  \n",
    "  It’s trained in a *“unified encoder-decoder”* way, meaning it can handle multiple speech and text tasks with the same architecture.  \n",
    "\n",
    "- **How it works in TTS**:  \n",
    "  1. **Text input** → tokenized by the `SpeechT5Processor`.  \n",
    "  2. **Encoder** processes the text tokens into hidden representations.  \n",
    "  3. **Decoder** generates **acoustic features** (like mel-spectrograms).  \n",
    "  4. **Vocoder (HiFi-GAN)** converts those spectrograms into actual **waveforms**.  \n",
    "  5. **Speaker embeddings** can be provided to control **voice style** (pitch, accent, timbre).  \n",
    "\n",
    "- **Key features**:  \n",
    "  - High-quality, natural-sounding speech synthesis.  \n",
    "  - Supports **speaker adaptation** → you can change voices with embeddings.  \n",
    "  - Part of the Hugging Face `transformers` library, easy to integrate.  \n",
    "\n",
    "- **Why you used it**:  \n",
    "  It gives you **state-of-the-art TTS** with relatively simple code, and lets you stream chunks of speech with character-level alignment.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac93ba4-62a1-47d1-8c4e-9197e7419ac8",
   "metadata": {},
   "source": [
    "### STEP 1: Load Models and Speaker Embeddings\n",
    "\n",
    "- **Processor**  \n",
    "  Handles text preprocessing and tokenization before passing into the model.\n",
    "\n",
    "- **Model (SpeechT5)**  \n",
    "  Converts processed text tokens into intermediate **mel-spectrograms** (representation of speech).\n",
    "\n",
    "- **Vocoder (SpeechT5HifiGan)**  \n",
    "  Converts mel-spectrograms into actual **audio waveforms** that we can play.\n",
    "\n",
    "- **Speaker Embeddings**  \n",
    "  - Pre-trained voice characteristics (from CMU Arctic dataset).  \n",
    "  - Index **7306** is used here to pick a specific voice.  \n",
    "  - These embeddings define **pitch, tone, accent, and timbre** of the generated speech.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d332c8-96ff-4ff8-a8d7-f211470b06e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|████████████████████████| 1/1 [00:00<00:00, 11618.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "import torch\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import websockets\n",
    "import json\n",
    "import base64\n",
    "import librosa\n",
    "\n",
    "# Load models\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90339194-3d2e-47b4-935c-329d117cf5fe",
   "metadata": {},
   "source": [
    "### STEP 2: Generate Audio Chunks with Character Timestamps\n",
    "\n",
    "This function processes text in chunks and generates synchronized audio:\n",
    "\n",
    "- **Chunking the text**  \n",
    "  Splits the input text into **4-word chunks** \n",
    "\n",
    "- **Generating speech for each chunk**  \n",
    "  - Passes tokens through the TTS model + vocoder to generate the audio waveform.\n",
    "\n",
    "- **Estimating character timings**  \n",
    "  - Calculates the total audio duration for the chunk.  \n",
    "  - Divides it equally across all characters (simple approximation).  \n",
    "\n",
    "- **Streaming output**  \n",
    "  Yields a dictionary containing:  \n",
    "  - Characters in the chunk  \n",
    "  - The generated audio tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb79c16-798d-4e6d-a5e8-b232e9464304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def generate_audio_chunks_with_char_timestamps(text: str, chunk_size=4):\n",
    "    words = text.strip().split()\n",
    "    char_pos = 0\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk_words = words[i:i+chunk_size]\n",
    "        chunk_text = \" \".join(chunk_words)\n",
    "        inputs = processor(text=chunk_text, return_tensors=\"pt\")\n",
    "        speech_chunk = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "        num_chars = len(chunk_text)\n",
    "        total_samples = speech_chunk.shape[0]\n",
    "        duration_ms = total_samples / 16000 * 1000\n",
    "        char_durations_ms = [duration_ms / num_chars] * num_chars\n",
    "        char_start_times_ms = np.cumsum([0]+char_durations_ms[:-1]).tolist()\n",
    "        yield {\n",
    "            \"chars\": list(chunk_text),\n",
    "            \"char_start_times_ms\": [round(t) for t in char_start_times_ms],\n",
    "            \"char_durations_ms\": [round(d) for d in char_durations_ms],\n",
    "            \"audio\": speech_chunk\n",
    "        }\n",
    "        char_pos += len(chunk_text) + 1\n",
    "        await asyncio.sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803edf98-c5ed-4525-b12d-d029f5130c71",
   "metadata": {},
   "source": [
    "### STEP 3: Conversion\n",
    "\n",
    "- **Model output**: The TTS model produces audio as a PyTorch tensor at 16kHz.  \n",
    "- **Playback compatibility**: Browsers expect **44.1kHz WAV/PCM**, so resampling is required.  \n",
    "\n",
    "➡️ In practice:  \n",
    "- Convert PyTorch tensor → NumPy array  \n",
    "- Resample from 16kHz → 44.1kHz  \n",
    "- Convert to 16-bit PCM integers  \n",
    "- Encode as Base64 for safe WebSocket transmission  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c2839a-2f63-41eb-b8f9-8c5cc1aca840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_base64(audio_tensor):\n",
    "    audio_np = audio_tensor.numpy()\n",
    "    audio_44k = librosa.resample(audio_np, orig_sr=16000, target_sr=44100)\n",
    "    audio_int16 = (audio_44k * 32767).astype(np.int16)\n",
    "    return base64.b64encode(audio_int16.tobytes()).decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e34db-437d-4bfd-9bc6-9b7ecf5f6512",
   "metadata": {},
   "source": [
    "### STEP 4: Handle WebSocket Client\n",
    "\n",
    "This function manages communication with the frontend UI:\n",
    "\n",
    "1. **Connection setup** – runs when a client connects, creates a `text_buffer`.  \n",
    "2. **Receiving messages** – gets `\"text\"` to add to the buffer, `\"flush\"` to trigger processing; `\" \"` signals start, `\"\"` signals end.  \n",
    "3. **Processing text** – on flush, generates audio chunks with timestamps, converts to Base64, and sends JSON back to the client.  \n",
    "4. **Keep alive** – clears the buffer for the next input; keeps the connection open unless closed or an error occurs.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b8b131-9b7b-47ba-bf46-55f05e60c510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server running on ws://localhost:8111\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 48] error while attempting to bind on address ('127.0.0.1', 8111): address already in use",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m start_server \u001b[38;5;241m=\u001b[39m websockets\u001b[38;5;241m.\u001b[39mserve(handle_client, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m8111\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServer running on ws://localhost:8111\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_server\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\u001b[38;5;241m.\u001b[39mrun_forever()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/orpheus_tts/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/orpheus_tts/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/orpheus_tts/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/orpheus_tts/lib/python3.11/asyncio/tasks.py:694\u001b[0m, in \u001b[0;36m_wrap_awaitable\u001b[0;34m(awaitable)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;129m@types\u001b[39m\u001b[38;5;241m.\u001b[39mcoroutine\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrap_awaitable\u001b[39m(awaitable):\n\u001b[1;32m    689\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Helper for asyncio.ensure_future().\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m    Wraps awaitable (an object with __await__) into a coroutine\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;124;03m    that will later be wrapped in a Task by ensure_future().\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01myield from\u001b[39;00m awaitable\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__await__\u001b[39m())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/orpheus_tts/lib/python3.11/site-packages/websockets/asyncio/server.py:831\u001b[0m, in \u001b[0;36mserve.__await_impl__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__await_impl__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Server:\n\u001b[0;32m--> 831\u001b[0m     server \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_server\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver\u001b[38;5;241m.\u001b[39mwrap(server)\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver\n",
      "File \u001b[0;32m/opt/anaconda3/envs/orpheus_tts/lib/python3.11/asyncio/base_events.py:1536\u001b[0m, in \u001b[0;36mBaseEventLoop.create_server\u001b[0;34m(self, protocol_factory, host, port, family, flags, sock, backlog, ssl, reuse_address, reuse_port, ssl_handshake_timeout, ssl_shutdown_timeout, start_serving)\u001b[0m\n\u001b[1;32m   1534\u001b[0m                 logger\u001b[38;5;241m.\u001b[39mwarning(msg)\n\u001b[1;32m   1535\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m-> 1536\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(err\u001b[38;5;241m.\u001b[39merrno, msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sockets:\n\u001b[1;32m   1539\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcould not bind on any address out of \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1540\u001b[0m                   \u001b[38;5;241m%\u001b[39m ([info[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m infos],))\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 48] error while attempting to bind on address ('127.0.0.1', 8111): address already in use"
     ]
    }
   ],
   "source": [
    "async def handle_client(websocket):\n",
    "    text_buffer = \"\"\n",
    "    print(\"Client connected\")\n",
    "    \n",
    "    try:\n",
    "        async for message in websocket:\n",
    "            print(f\"Received: {message}\")\n",
    "            data = json.loads(message)\n",
    "            text = data[\"text\"]\n",
    "            flush = data[\"flush\"]\n",
    "            \n",
    "            if text == \" \":\n",
    "                print(\"Got initial space\")\n",
    "                continue\n",
    "            if text == \"\":\n",
    "                print(\"Got final empty\")\n",
    "                break\n",
    "                \n",
    "            text_buffer += text\n",
    "            print(f\"Text buffer: {text_buffer}\")\n",
    "            \n",
    "            if flush:\n",
    "                print(\"Processing flush...\")\n",
    "                async for chunk_data in generate_audio_chunks_with_char_timestamps(text_buffer):\n",
    "                    print(\"Got chunk, converting audio...\")\n",
    "                    audio_b64 = audio_to_base64(chunk_data[\"audio\"])\n",
    "                    response = {\n",
    "                        \"audio\": audio_b64,\n",
    "                        \"alignment\": {\n",
    "                            \"chars\": chunk_data[\"chars\"],\n",
    "                            \"char_start_times_ms\": chunk_data[\"char_start_times_ms\"],\n",
    "                            \"char_durations_ms\": chunk_data[\"char_durations_ms\"]\n",
    "                        }\n",
    "                    }\n",
    "                    print(\"Sending response...\")\n",
    "                    await websocket.send(json.dumps(response))\n",
    "                text_buffer = \"\"\n",
    "                print(\"Done processing - connection stays open\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "start_server = websockets.serve(handle_client, \"localhost\", 8111)\n",
    "print(\"Server running on ws://localhost:8111\")\n",
    "asyncio.get_event_loop().run_until_complete(start_server)\n",
    "asyncio.get_event_loop().run_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d22ae1-b72c-4a64-9195-f8f5d2253545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orpheus TTS",
   "language": "python",
   "name": "orpheus_tts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
